{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Foundations of Data Science HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Ruchita Paithankar\n",
    "    - Email: \n",
    "- Group member 2\n",
    "    - Name:\n",
    "    - Email: \n",
    "- Group member 3\n",
    "    - Name: \n",
    "    - Email: \n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing chain rule in Artificial neural network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with one input and one output neuron. \n",
    "From the class, recall \n",
    "$$a^{(1)}=\\sigma(z^{(1)})$$\n",
    "$$z^{(1)} = w^{(1)}a^{(0)}+b^{(1)}$$\n",
    "\n",
    "We can find how good or bad our neural network is based on a cost function $C_k$ for $k$th training example.\n",
    "$$C_k = (a^{(1)}-y)^2$$\n",
    "Later we will see how to apply this to an entire set of training examples. But now let us look at an instance where $x=1$ and $y=0$, i.e., for $x=1$ input the label is $y=1$. Consider the starting weight $w^{(1)}= 1.3$ and bias parameter $b^{(1)}=-0.1$. Using the below code, calculate $a^{(1)}$ and then the cost $C_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02767078976828052\n"
     ]
    }
   ],
   "source": [
    "# First we set the state of the network\n",
    "σ = np.tanh\n",
    "w1 = 1.3\n",
    "b1 = -0.1\n",
    "\n",
    "# Then we define the neuron activation.\n",
    "def a1(a0):\n",
    "    z = w1 * a0 + b1\n",
    "    return σ(z)\n",
    "\n",
    "# Experiment with different values of x below.\n",
    "x = 1\n",
    "print((a1(x)-1)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function of a training set is the average of the individual cost functions of the data in the training set,\n",
    "\n",
    "$$C = \\frac{1}{N} \\sum_k C_k$$ where $N$ is the total number of training examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the neural network on the training data, we can vary the weight and bias. We can calculate the derivative of the example cost with respect to these quantities using the chain rule.\n",
    "\n",
    "$$\\frac{\\partial C_k}{\\partial w^{(1)}} = \\frac{\\partial C_k}{\\partial a^{(1)}}\\frac{\\partial a^{(1)}}{\\partial z^{(1)}}\\frac{\\partial z^{(1)}}{\\partial w^{(1)}}$$\n",
    "$$\\frac{\\partial C_k}{\\partial b^{(1)}} = \\frac{\\partial C_k}{\\partial a^{(1)}}\\frac{\\partial a^{(1)}}{\\partial z^{(1)}}\\frac{\\partial z^{(1)}}{\\partial b^{(1)}}$$\n",
    "Individually, these derivatives take fairly simple form. Go ahead and calculate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $$\\sigma(z) = tanh(z)$$ \n",
    "$$\\sigma^{'}(z) = 1/cosh^{2}(z) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your task is to replace ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define our sigma function.\n",
    "sigma = np.tanh\n",
    "\n",
    "# Next define the feed-forward equation.\n",
    "def a1 (w1, b1, a0) :\n",
    "    z = w1 * a0 + b1\n",
    "    return sigma(z)\n",
    "\n",
    "# The individual cost function is the square of the difference between\n",
    "# the network output and the training data output.\n",
    "def C (w1, b1, x, y) :\n",
    "    return (a1(w1, b1, x) - y)**2\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the weight.\n",
    "def dCdw (w1, b1, x, y) :\n",
    "    z = w1 * x + b1\n",
    "    dCda = 2 * (a1(w1, b1, x) - y) # Derivative of cost with activation\n",
    "    dadz = 1/np.cosh(z)**2 # derivative of activation with weighted sum z\n",
    "    dzdw = x # derivative of weighted sum z with weight\n",
    "    return dCda * dadz * dzdb # Return the chain rule product.\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the bias.\n",
    "# It is very similar to the previous function.\n",
    "# You should complete this function.\n",
    "def dCdb (w1, b1, x, y) :\n",
    "    z = w1 * x + b1\n",
    "    dCda = 2 * (a1(w1, b1, x) - y)\n",
    "    dadz = 1/np.cosh(z)**2\n",
    "    # Change the next line to give the derivative of\n",
    "     # the weighted sum, z, with respect to the bias, b.\n",
    " \n",
    "    dzdb = 1\n",
    "    return dCda * dadz * dzdb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1186026425530913\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "# Let's start with an unfit weight and bias.\n",
    "\n",
    "w1 = 2.3\n",
    "b1 = -1.2\n",
    "# We can test on a single data point pair of x and y.\n",
    "x = 0\n",
    "y = 1\n",
    "# Output how the cost would change\n",
    "# in proportion to a small change in the bias\n",
    "print( dCdb(w1, b1, x, y) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Sanity check\n",
    "# Let's start with an unfit weight and bias.\n",
    "\n",
    "w1 = 2.3\n",
    "b1 = -1.2\n",
    "# We can test on a single data point pair of x and y.\n",
    "x = 0\n",
    "y = 1\n",
    "# Output how the cost would change\n",
    "# in proportion to a small change in the bias\n",
    "print( dCdb(w1, b1, x, y) )\n",
    "\n",
    "You should get -1.1186026425530913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real world, you will have a large number of training examples and hence use matrix or vector operations instead of scalar multiplications.\n",
    "You are given the weight matrix $W$ and bias vector $b$. Complete the code for finding $a1, C, dCdW and dCdb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function.\n",
    "sigma = np.tanh\n",
    "\n",
    "# Let's use a random initial weight and bias.\n",
    "W = np.array([[-0.94529712, -0.2667356 , -0.91219181],\n",
    "              [ 2.05529992,  1.21797092,  0.22914497]])\n",
    "b = np.array([ 0.61273249,  1.6422662 ])\n",
    "\n",
    "# define our feed forward function\n",
    "def a1 (a0) :\n",
    "  # Notice the next line is almost the same as previously,\n",
    "  # except we are using matrix multiplication rather than scalar multiplication\n",
    "  z = W @ a0 + b\n",
    "  # Everything else is the same though,\n",
    "  return sigma(z)\n",
    "\n",
    "# Next, if a training example is,\n",
    "x = np.array([0.7, 0.6, 0.2])\n",
    "y = np.array([0.9, 0.6])\n",
    "\n",
    "# Then the cost function is,\n",
    "d = a1(x) - y # Vector difference between observed and expected activation\n",
    "C = d @ d # Absolute value squared of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.19184549e+00,  1.42277240e-03])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First define our sigma function.\n",
    "sigma = np.tanh\n",
    "\n",
    "# Next define the feed-forward equation.\n",
    "def a1 (w1, b1, a0) :\n",
    "    z = np.matmul(w1 , a0) + b1\n",
    "    return sigma(z)\n",
    "\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the weight.\n",
    "def dCdw (w1, b1, x, y) :\n",
    "   \n",
    "    dCda = 2 * (a1(w1, b1, x) - y) # Derivative of cost with activation\n",
    "    \n",
    "    dadz = 1/np.square(np.cosh(np.matmul(w1,x) + b1)) # derivative of activation with weighted sum z\n",
    "\n",
    "    J = (dCda * dadz).reshape((2,1))\n",
    "\n",
    "    dzdw = x # derivative of weighted sum z with weight\n",
    "\n",
    "    J = J * dzdw\n",
    "    \n",
    "    return J # Return the chain rule product.\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the bias.\n",
    "# It is very similar to the previous function.\n",
    "# You should complete this function.\n",
    "def dCdb (w1, b1, x, y) :\n",
    "    \n",
    "    dCda = 2 * (a1(w1, b1, x) - y)\n",
    "    dadz = 1/np.cosh(w1 @ x + b1)**2\n",
    "    # Change the next line to give the derivative of\n",
    "     # the weighted sum, z, with respect to the bias, b.\n",
    " \n",
    "    dzdb = 1\n",
    "    return dCda * dadz * dzdb\n",
    "\n",
    "dCdb (W, b, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.53429185e+00, -1.31510730e+00, -4.38369099e-01],\n",
       "       [ 9.95940681e-04,  8.53663441e-04,  2.84554480e-04]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dCdw (W, b, x, y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#sanity check\n",
    "\n",
    "dCdb (W, b, x, y)\n",
    "\n",
    "array([[-2.19184549e+00],\n",
    "       [ 1.42277240e-03]])\n",
    "       \n",
    "dCdw (W, b, x, y)\n",
    "\n",
    "array([[-1.53429185e+00, -1.31510730e+00, -4.38369099e-01],\n",
    "       [ 9.95940681e-04,  8.53663441e-04,  2.84554480e-04]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
